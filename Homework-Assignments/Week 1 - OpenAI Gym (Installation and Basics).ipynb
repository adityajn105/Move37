{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym\n",
    "1. Gym is a toolkit for developing and comparing reinforcement learning algorithms. \n",
    "2. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CartPole Env\n",
    "1. CartPole is the most basic environment of all types.\n",
    "2. The problem consists of balancing a pole connected with one joint on top of a moving cart. The only actions are to add a force of -1 or +1 to the cart, pushing it left or right.\n",
    "3. In CartPole's environment, there are four observations at any given state, representing information such as the angle of the pole and the position of the cart.\n",
    "4. Using these observations, the agent needs to decide on one of two possible actions: move the cart left or right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "def run_episode(env, parameters, play=False): \n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            if play: env.close()\n",
    "            break\n",
    "        else:\n",
    "            if play: env.render()\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 1 : Random Search\n",
    "1. One fairly straightforward strategy is to keep trying random weights, and pick the one that performs the best.\n",
    "2. Since the CartPole environment is relatively simple, with only 4 observations, this basic method works surprisingly well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.19724541, -2.29519346,  0.3464877 ,  3.70601063]), 0.0, True, {})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.02799342, -0.00475672,  0.03632073, -0.01977023])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look what taking an action returns\n",
    "print(env.step(0))\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Reward : 200.0 in 17 iterations\n"
     ]
    }
   ],
   "source": [
    "best_parameters = np.random.rand(4) * 2 - 1\n",
    "best_reward = run_episode(env,best_parameters,False)\n",
    "iters = 0\n",
    "for _ in range(1000):\n",
    "    parameters =  np.random.rand(4) * 2 - 1\n",
    "    reward = run_episode(env,parameters,False)\n",
    "    if reward > best_reward:\n",
    "        best_parameters = parameters\n",
    "        best_reward = reward\n",
    "    iters+=1\n",
    "    if best_reward==200:\n",
    "        break\n",
    "print(\"Highest Reward : {} in {} iterations\".format(best_reward,iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 2 : Hill Climbing\n",
    "1. We start with some randomly chosen initial weights. Every episode, add some noise to the weights, and keep the new weights if the agent improves.\n",
    "2. Idea here is to gradually improve the weights, rather than keep jumping around and hopefully finding some combination that works. If noise_scaling is high enough in comparison to the current weights, this algorithm is essentially the same as random search.\n",
    "3. If the weights are initialized badly, adding noise may have no effect on how well the agent performs, causing it to get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Reward : 200.0 in 38 iterations\n"
     ]
    }
   ],
   "source": [
    "noise_scaling = 0.1\n",
    "best_parameters = np.random.rand(4)*2 - 1\n",
    "best_reward = 0\n",
    "last_update = 0\n",
    "iters = 0\n",
    "while last_update < 100:\n",
    "    parameters = best_parameters + (np.random.rand(4)*2 - 1)*noise_scaling\n",
    "    reward = run_episode(env,parameters,False)\n",
    "    if reward > best_reward:\n",
    "        best_reward = reward\n",
    "        best_parameters = parameters\n",
    "        last_update = 0\n",
    "    else:\n",
    "        last_update+=1\n",
    "        \n",
    "    #incrementing iterations\n",
    "    iters += 1\n",
    "    \n",
    "    if best_reward==200:\n",
    "        break\n",
    "print(\"Highest Reward : {} in {} iterations\".format(best_reward,iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
