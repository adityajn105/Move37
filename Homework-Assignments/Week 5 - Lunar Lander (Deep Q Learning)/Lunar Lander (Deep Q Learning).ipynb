{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJG1tV3p-8EM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/.conda/envs/py3k/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "K-dCPXDq_Gj1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_features, n_actions):\n",
    "        super(DQN,self).__init__()\n",
    "        self.neuralnet = nn.Sequential(\n",
    "            nn.Linear(in_features,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.neuralnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B6zGowe__ac8"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "class ExperienceBuffer():\n",
    "    def __init__(self,capacity):\n",
    "        self.exp_buffer = collections.deque(maxlen=capacity)\n",
    "        \n",
    "    def append(self,exp):\n",
    "        self.exp_buffer.append(exp)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.exp_buffer)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.exp_buffer.clear()\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        indices = np.random.choice( range(len(self.exp_buffer)), batch_size )\n",
    "        states,actions,rewards,dones,next_states = zip(*[self.exp_buffer[i] for i in indices])\n",
    "        return np.array(states),np.array(actions),np.array(rewards, dtype=np.float32),np.array(dones,dtype=np.uint8),np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "G8ZC4Jbc_oo6"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,env,buffer):\n",
    "        self.env = env\n",
    "        self.buffer = buffer\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_rewards = 0.0\n",
    "    \n",
    "    def step(self, net, eps, device=\"cpu\"):\n",
    "        done_reward= None\n",
    "        if np.random.random() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_prev = torch.tensor(self.state).to(device)\n",
    "            action = int(torch.argmax(net(state_prev).to(device)))\n",
    "            \n",
    "        state_prev = self.state\n",
    "        rewards = 0\n",
    "        done = False\n",
    "        for _ in range(4):\n",
    "            self.state,reward,done,info = env.step(action)\n",
    "            self.total_rewards+=reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        self.buffer.append((state_prev,action,reward,done,self.state))\n",
    "        if done:\n",
    "            done_reward = self.total_rewards\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "U5sWRHaU_fD7"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "EPSILON_START = 1\n",
    "EPSILON_FINAL = 0.01\n",
    "EPSILON_DECAY_OBS = 10**5\n",
    "BATCH_SIZE = 32\n",
    "MEAN_GOAL_REWARD = 250\n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "REPLAY_MIN_SIZE = 10000\n",
    "LEARNING_RATE= 1e-4\n",
    "SYNC_TARGET_OBS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iEc5pGIqAGB8"
   },
   "outputs": [],
   "source": [
    "def cal_loss(batch, net, tgt_net, device='cpu'):\n",
    "    states,actions,rewards,dones,next_states = batch\n",
    "    \n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    dones_v = torch.ByteTensor(dones).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    \n",
    "    \n",
    "    Q_val = net(states_v).gather(1,actions_v.unsqueeze(-1)).squeeze(-1) #select q value corresponding each action\n",
    "    Q_val_next = tgt_net(next_states_v).max(1)[0] #give maximum value for each sample\n",
    "    Q_val_next[dones_v] = 0.0 #making q value for done to zero\n",
    "    Q_val_next = Q_val_next.detach() #detach from current graph\n",
    "    \n",
    "    expected_return = rewards_v + GAMMA * Q_val_next #what should be\n",
    "    return nn.MSELoss()(Q_val,expected_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1482
    },
    "colab_type": "code",
    "id": "NdXuc3WBAjbi",
    "outputId": "a566c52e-d710-4d8c-df94-a7d76324ebe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME : 91, TIME ECLAPSED : 0.0198667049407959, EPSILON : 0.99909, MEAN_REWARD : -236.57854038856965\n",
      "Reward -272.3648135666385 -> -236.57854038856965 Model Saved\n",
      "GAME : 113, TIME ECLAPSED : 0.016995668411254883, EPSILON : 0.99887, MEAN_REWARD : -227.97405359419668\n",
      "Reward -236.57854038856965 -> -227.97405359419668 Model Saved\n",
      "GAME : 143, TIME ECLAPSED : 0.01102590560913086, EPSILON : 0.99857, MEAN_REWARD : -186.95619094043488\n",
      "Reward -207.1112979943246 -> -186.95619094043488 Model Saved\n",
      "GAME : 159, TIME ECLAPSED : 0.012368440628051758, EPSILON : 0.99841, MEAN_REWARD : -172.4161077995954\n",
      "Reward -186.95619094043488 -> -172.4161077995954 Model Saved\n",
      "GAME : 25135, TIME ECLAPSED : 0.34514451026916504, EPSILON : 0.74865, MEAN_REWARD : -170.9960655080618\n",
      "Reward -172.4161077995954 -> -170.9960655080618 Model Saved\n",
      "GAME : 25751, TIME ECLAPSED : 0.1059412956237793, EPSILON : 0.74249, MEAN_REWARD : -161.6702902492795\n",
      "Reward -161.99398269200546 -> -161.6702902492795 Model Saved\n",
      "GAME : 26287, TIME ECLAPSED : 0.18701410293579102, EPSILON : 0.7371300000000001, MEAN_REWARD : -149.8530630941037\n",
      "Reward -150.64792731691824 -> -149.8530630941037 Model Saved\n",
      "GAME : 26759, TIME ECLAPSED : 0.2164924144744873, EPSILON : 0.73241, MEAN_REWARD : -135.0639745851816\n",
      "Reward -137.36348869646957 -> -135.0639745851816 Model Saved\n",
      "GAME : 30397, TIME ECLAPSED : 0.15085554122924805, EPSILON : 0.6960299999999999, MEAN_REWARD : -126.62564976750402\n",
      "Reward -126.8127161892147 -> -126.62564976750402 Model Saved\n",
      "GAME : 41147, TIME ECLAPSED : 0.20228314399719238, EPSILON : 0.58853, MEAN_REWARD : -114.57119858025258\n",
      "Reward -115.582719304681 -> -114.57119858025258 Model Saved\n",
      "GAME : 49240, TIME ECLAPSED : 0.30147266387939453, EPSILON : 0.5076, MEAN_REWARD : -103.57712709729424\n",
      "Reward -104.9738417170605 -> -103.57712709729424 Model Saved\n",
      "GAME : 52436, TIME ECLAPSED : 0.31389808654785156, EPSILON : 0.47563999999999995, MEAN_REWARD : -93.96741200921753\n",
      "Reward -94.23511982269656 -> -93.96741200921753 Model Saved\n",
      "GAME : 54958, TIME ECLAPSED : 0.7996954917907715, EPSILON : 0.45042000000000004, MEAN_REWARD : -82.14302355240542\n",
      "Reward -84.19265086314837 -> -82.14302355240542 Model Saved\n",
      "GAME : 80168, TIME ECLAPSED : 3.516799211502075, EPSILON : 0.19832000000000005, MEAN_REWARD : -70.64932098421488\n",
      "Reward -73.26251958460936 -> -70.64932098421488 Model Saved\n",
      "GAME : 83030, TIME ECLAPSED : 2.6227433681488037, EPSILON : 0.16969999999999996, MEAN_REWARD : -59.74154595248005\n",
      "Reward -60.88825824573486 -> -59.74154595248005 Model Saved\n",
      "GAME : 84745, TIME ECLAPSED : 2.4645423889160156, EPSILON : 0.15254999999999996, MEAN_REWARD : -47.65106492857802\n",
      "Reward -50.618240965928386 -> -47.65106492857802 Model Saved\n",
      "GAME : 85614, TIME ECLAPSED : 0.539750337600708, EPSILON : 0.14386, MEAN_REWARD : -39.57175751442898\n",
      "Reward -39.889889008226895 -> -39.57175751442898 Model Saved\n",
      "GAME : 86608, TIME ECLAPSED : 2.012617588043213, EPSILON : 0.13392000000000004, MEAN_REWARD : -27.18726998106614\n",
      "Reward -29.038337682196435 -> -27.18726998106614 Model Saved\n",
      "GAME : 87239, TIME ECLAPSED : 2.1654481887817383, EPSILON : 0.12761, MEAN_REWARD : -17.683224696290427\n",
      "Reward -18.06494456984504 -> -17.683224696290427 Model Saved\n",
      "GAME : 88082, TIME ECLAPSED : 0.777045726776123, EPSILON : 0.11917999999999995, MEAN_REWARD : -4.268393949830494\n",
      "Reward -7.1465569159149505 -> -4.268393949830494 Model Saved\n",
      "GAME : 88560, TIME ECLAPSED : 2.047680139541626, EPSILON : 0.11439999999999995, MEAN_REWARD : 6.505699730612639\n",
      "Reward 4.258686941949164 -> 6.505699730612639 Model Saved\n",
      "GAME : 89294, TIME ECLAPSED : 1.0613815784454346, EPSILON : 0.10706000000000004, MEAN_REWARD : 18.309406497353788\n",
      "Reward 15.872101686636007 -> 18.309406497353788 Model Saved\n",
      "GAME : 89854, TIME ECLAPSED : 0.6994202136993408, EPSILON : 0.10146, MEAN_REWARD : 29.460478692577713\n",
      "Reward 27.049208778694673 -> 29.460478692577713 Model Saved\n",
      "GAME : 90268, TIME ECLAPSED : 0.7075512409210205, EPSILON : 0.1, MEAN_REWARD : 41.987161161872265\n",
      "Reward 38.998293874364876 -> 41.987161161872265 Model Saved\n",
      "GAME : 90647, TIME ECLAPSED : 0.6657531261444092, EPSILON : 0.1, MEAN_REWARD : 52.15086075674879\n",
      "Reward 49.49782868906137 -> 52.15086075674879 Model Saved\n",
      "GAME : 91159, TIME ECLAPSED : 0.7375674247741699, EPSILON : 0.1, MEAN_REWARD : 64.02677577749797\n",
      "Reward 60.727138811891336 -> 64.02677577749797 Model Saved\n",
      "GAME : 91692, TIME ECLAPSED : 0.6593880653381348, EPSILON : 0.1, MEAN_REWARD : 77.77897284051824\n",
      "Reward 73.16430450640051 -> 77.77897284051824 Model Saved\n",
      "GAME : 92149, TIME ECLAPSED : 0.6556046009063721, EPSILON : 0.1, MEAN_REWARD : 87.656130083107\n",
      "Reward 85.11775475139235 -> 87.656130083107 Model Saved\n",
      "GAME : 92941, TIME ECLAPSED : 1.8299224376678467, EPSILON : 0.1, MEAN_REWARD : 99.71845739630582\n",
      "Reward 96.5572665934404 -> 99.71845739630582 Model Saved\n",
      "GAME : 93828, TIME ECLAPSED : 1.1661889553070068, EPSILON : 0.1, MEAN_REWARD : 111.1671536212968\n",
      "Reward 108.09398194761864 -> 111.1671536212968 Model Saved\n",
      "GAME : 94323, TIME ECLAPSED : 0.5608971118927002, EPSILON : 0.1, MEAN_REWARD : 122.37164342082399\n",
      "Reward 119.82558378223607 -> 122.37164342082399 Model Saved\n",
      "GAME : 94979, TIME ECLAPSED : 0.7402188777923584, EPSILON : 0.1, MEAN_REWARD : 133.96706859215158\n",
      "Reward 131.13428063211043 -> 133.96706859215158 Model Saved\n",
      "GAME : 95839, TIME ECLAPSED : 0.42258763313293457, EPSILON : 0.1, MEAN_REWARD : 141.91943486665625\n",
      "Reward 141.5352137888348 -> 141.91943486665625 Model Saved\n",
      "GAME : 97659, TIME ECLAPSED : 0.3613407611846924, EPSILON : 0.1, MEAN_REWARD : 153.7500865028455\n",
      "Reward 153.54606138998287 -> 153.7500865028455 Model Saved\n",
      "GAME : 98976, TIME ECLAPSED : 0.5092611312866211, EPSILON : 0.1, MEAN_REWARD : 164.26459380637675\n",
      "Reward 163.725364382309 -> 164.26459380637675 Model Saved\n",
      "GAME : 103078, TIME ECLAPSED : 0.4192695617675781, EPSILON : 0.1, MEAN_REWARD : 180.84577949036077\n",
      "Reward 177.33282872079434 -> 180.84577949036077 Model Saved\n",
      "GAME : 104357, TIME ECLAPSED : 0.3525261878967285, EPSILON : 0.1, MEAN_REWARD : 192.92710669874486\n",
      "Reward 191.6208323172188 -> 192.92710669874486 Model Saved\n",
      "GAME : 156278, TIME ECLAPSED : 0.781919002532959, EPSILON : 0.1, MEAN_REWARD : 207.25531844148725\n",
      "Reward 204.11263895370408 -> 207.25531844148725 Model Saved\n",
      "GAME : 156904, TIME ECLAPSED : 0.6971778869628906, EPSILON : 0.1, MEAN_REWARD : 216.50094327158237\n",
      "Reward 216.14822741452554 -> 216.50094327158237 Model Saved\n",
      "GAME : 159373, TIME ECLAPSED : 0.42975640296936035, EPSILON : 0.1, MEAN_REWARD : 226.4580843315098\n",
      "Reward 226.33350874929405 -> 226.4580843315098 Model Saved\n",
      "GAME : 160595, TIME ECLAPSED : 0.34926509857177734, EPSILON : 0.1, MEAN_REWARD : 237.3513033485341\n",
      "Reward 237.0878316770252 -> 237.3513033485341 Model Saved\n",
      "GAME : 351650, TIME ECLAPSED : 0.39455556869506836, EPSILON : 0.1, MEAN_REWARD : 248.17286421073013\n",
      "Reward 247.91707358040495 -> 248.17286421073013 Model Saved\n",
      "SOLVED in 352390 obs\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = DQN(env.observation_space.shape[0],env.action_space.n).to(device)\n",
    "tgt_net = DQN(env.observation_space.shape[0],env.action_space.n).to(device)\n",
    "\n",
    "buffer= ExperienceBuffer(REPLAY_BUFFER_SIZE)\n",
    "\n",
    "agent = Agent(env,buffer)\n",
    "epsilon = EPSILON_START\n",
    "optimizer = optim.Adam(net.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "total_rewards= []\n",
    "\n",
    "ts = time.time()\n",
    "best_mean_reward= None\n",
    "obs_id = 0\n",
    "\n",
    "while True:\n",
    "    obs_id +=1\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - obs_id/EPSILON_DECAY_OBS)\n",
    "    \n",
    "    reward = agent.step(net,epsilon,device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        game_time = time.time() - ts\n",
    "        ts = time.time()\n",
    "        mean_reward=  np.mean(total_rewards[-100:])\n",
    "        \n",
    "        if best_mean_reward == None or best_mean_reward < mean_reward:\n",
    "            torch.save(net.state_dict(),'checkpoints/lunar_lander-best.dat')\n",
    "            \n",
    "            if best_mean_reward == None:\n",
    "                last = mean_reward\n",
    "                best_mean_reward = mean_reward\n",
    "            \n",
    "            if best_mean_reward is not None and best_mean_reward - last > 10:\n",
    "                last = best_mean_reward\n",
    "                print(\"GAME : {}, TIME ECLAPSED : {}, EPSILON : {}, MEAN_REWARD : {}\".format(obs_id,game_time,epsilon,mean_reward))\n",
    "                print(\"Reward {} -> {} Model Saved\".format(best_mean_reward,mean_reward))\n",
    "            \n",
    "            best_mean_reward = mean_reward\n",
    "        \n",
    "        if mean_reward > MEAN_GOAL_REWARD:\n",
    "            print(\"SOLVED in {} obs\".format(obs_id))\n",
    "            break\n",
    "        \n",
    "    if len(buffer) < REPLAY_MIN_SIZE:\n",
    "        continue\n",
    "        \n",
    "    if obs_id % SYNC_TARGET_OBS == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = cal_loss(batch,net,tgt_net,device= device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jCgBTTCgehVC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "temp.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
