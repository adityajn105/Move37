{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "1. It contains 4 columns and 3 rows\n",
    "2. Position 1,4 has Queen\n",
    "3. Position 2,4 has firepit\n",
    "4. Position 2,2 has wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Our Grid Object\n",
    "\"\"\"\n",
    "class Grid:\n",
    "    def __init__(self,width,height,start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0] #vertical axis\n",
    "        self.j = start[1] #horizontal axis\n",
    "    \n",
    "    def set(self,actions,rewards,obey_prob):\n",
    "        self.actions = actions # dict:  (row,col): list of actions\n",
    "        self.rewards = rewards # dict:  (row,col):  reward\n",
    "        self.obey_prob = obey_prob\n",
    "    \n",
    "    def non_terminal_states(self):\n",
    "        return self.actions.keys()\n",
    "    \n",
    "    def set_state(self,s):\n",
    "        #change current stae\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "    \n",
    "    def current_state(self):\n",
    "        return (self.i,self.j)\n",
    "    \n",
    "    def is_terminal(self,s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def check_move(self,action):\n",
    "        i = self.i\n",
    "        j = self.j\n",
    "        #check move validity\n",
    "        if action in self.actions[(i,j)]:\n",
    "            if action == \"U\":\n",
    "                i-=1\n",
    "            elif action == \"D\":\n",
    "                i+=1\n",
    "            elif action == \"R\":\n",
    "                j+=1\n",
    "            elif action == \"L\":\n",
    "                j-=1\n",
    "                \n",
    "        reward = self.rewards.get((i,j),0)\n",
    "        return ((i,j),reward)\n",
    "    \n",
    "    def get_transition_probs(self,action):\n",
    "        # returns a list of (probability, reward, s') transition tuples\n",
    "        probs = []\n",
    "        state, reward = self.check_move(action)\n",
    "        probs.append((self.obey_prob,reward,state))\n",
    "        \n",
    "        \"\"\"\n",
    "        if obey prob is 0.8\n",
    "        and if action is 'up' then \n",
    "        'up' has probabiltiy of 0.8\n",
    "        'left' has probability of 0.1\n",
    "        'right' has probability of 0.1\n",
    "        \"\"\"\n",
    "        disobey_prob = 1-self.obey_prob\n",
    "        if disobey_prob <= 0:\n",
    "            return probs\n",
    "        if action == \"U\" or action == \"D\":\n",
    "            state, reward = self.check_move(\"L\")\n",
    "            probs.append((disobey_prob/2,reward,state))\n",
    "            state, reward = self.check_move(\"R\")\n",
    "            probs.append((disobey_prob/2,reward,state))\n",
    "        elif action == \"L\" or action == \"R\":\n",
    "            state, reward = self.check_move(\"U\")\n",
    "            probs.append((disobey_prob/2,reward,state))\n",
    "            state,reward = self.check_move(\"D\")\n",
    "            probs.append((disobey_prob/2,reward,state))\n",
    "        return probs\n",
    "    \n",
    "    def game_over(self):\n",
    "        #return True if game is over else False\n",
    "        #true if we are in state where no action are possible\n",
    "        return (self.i,self.j) not in self.actions.keys()\n",
    "    \n",
    "    def all_states(self):\n",
    "        # get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(self.actions.keys()) | set(self.rewards.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialization of Grid\n",
    "\"\"\"\n",
    "def standard_grid(obey_prob=0.1, step_cost = None):\n",
    "    # lets define a grid that describes the reward for getting into each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    # obey_brob (float): the probability of obeying the command\n",
    "    # step_cost (float): a penalty applied each step to minimize the number of moves (-0.1)\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    \n",
    "    rewards = {(0,3):+1, (1,3):-1}\n",
    "    start = (2,0)\n",
    "    actions = {\n",
    "        (0,0) : [\"R\",\"D\"],\n",
    "        (0,1) : [\"L\",\"R\"],\n",
    "        (0,2) : [\"L\",\"D\",\"R\"],\n",
    "        (1,0) : [\"U\",\"D\"],\n",
    "        (1,2) : [\"U\",\"D\",\"R\"],\n",
    "        (2,0) : [\"U\",\"R\"],\n",
    "        (2,1) : [\"L\",\"R\"],\n",
    "        (2,2) : [\"L\",\"U\",\"R\"],\n",
    "        (2,3) : [\"L\",\"U\"]\n",
    "    }\n",
    "    \n",
    "    g.set(actions,rewards,obey_prob)\n",
    "    if step_cost is not None:\n",
    "        g.rewards.update({\n",
    "          (0, 0): step_cost,\n",
    "          (0, 1): step_cost,\n",
    "          (0, 2): step_cost,\n",
    "          (1, 0): step_cost,\n",
    "          (1, 2): step_cost,\n",
    "          (2, 0): step_cost,\n",
    "          (2, 1): step_cost,\n",
    "          (2, 2): step_cost,\n",
    "          (2, 3): step_cost,\n",
    "        })\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Displaying Results\n",
    "\"\"\"\n",
    "def print_values(V, g):\n",
    "    for i in range(g.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.height):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "        print(\"\")\n",
    "\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.height):\n",
    "            a = P.get((i,j), ' ')\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Value-Iteration\n",
    "\"\"\"\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = [\"U\",\"D\",\"R\",\"L\"]\n",
    "\n",
    "def best_action_value(grid,V,s):\n",
    "    # finds the highest value action (max_a) from state s, returns the action and value\n",
    "    best_a = None\n",
    "    best_value = float('-inf')\n",
    "    grid.set_state(s)\n",
    "    \n",
    "    #loop through all possible action to find the best possible actions\n",
    "    for action in ALL_POSSIBLE_ACTIONS:\n",
    "        transitions = grid.get_transition_probs(action)\n",
    "        expected_v = 0\n",
    "        expected_r = 0\n",
    "        for (prob,r,state_prime) in transitions:\n",
    "            expected_v += prob*V[state_prime]\n",
    "            expected_r += prob*r\n",
    "        v = expected_r + GAMMA* expected_v\n",
    "        if v > best_value:\n",
    "            best_value = v\n",
    "            best_a = action\n",
    "    return (best_a,best_value)\n",
    "\n",
    "\n",
    "def calculate_values(grid):\n",
    "    V = {}\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "        V[s] = 0\n",
    "    \n",
    "    #repeat until convergence\n",
    "    #V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + gamma*V[s']] } }\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        for s in grid.non_terminal_states():\n",
    "            old_v = V[s]\n",
    "            _,new_v = best_action_value(grid,V,s)\n",
    "            V[s] = new_v\n",
    "            biggest_change = max(biggest_change,np.abs(old_v-new_v))\n",
    "        if biggest_change <= SMALL_ENOUGH:\n",
    "            break;\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_random_policy():\n",
    "    # policy is a lookup table for state -> action\n",
    "    # we'll randomly choose an action and update as we learn\n",
    "    policy = {}\n",
    "    for s in grid.non_terminal_states():\n",
    "        policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def calculate_greedy_policy(grid, V):\n",
    "    policy = initialize_random_policy()\n",
    "    # find a policy that leads to optimal value function\n",
    "    for s in policy.keys():\n",
    "        grid.set_state(s)\n",
    "        # loop through all possible actions to find the best current action\n",
    "        best_a, _ = best_action_value(grid, V, s)\n",
    "        policy[s] = best_a\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_game(grid):\n",
    "    # print rewards\n",
    "    print(\"rewards:\")\n",
    "    print_values(grid.rewards, grid)\n",
    "\n",
    "    # calculate accurate values for each square\n",
    "    V = calculate_values(grid)\n",
    "\n",
    "    # calculate the optimum policy based on our values\n",
    "    policy = calculate_greedy_policy(grid, V)\n",
    "\n",
    "    # our goal here is to verify that we get the same answer as with policy iteration\n",
    "    print(\"\\nvalues:\")\n",
    "    print_values(V, grid)\n",
    "    print(\"\\npolicy:\")\n",
    "    print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "\n",
      "values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "grid = standard_grid(obey_prob=1.0, step_cost=None)\n",
    "print_game(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### obey_prob = 1.0 | step_down = None\n",
    "Mario is not drunk it will follow what it said, it is most simple one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "\n",
      "values:\n",
      "---------------------------\n",
      " 0.72| 0.83| 0.94| 0.00|\n",
      "---------------------------\n",
      " 0.63| 0.00| 0.64| 0.00|\n",
      "---------------------------\n",
      " 0.54| 0.48| 0.53| 0.31|\n",
      "\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "grid = standard_grid(obey_prob=0.8, step_cost=None)\n",
    "print_game(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### obey_prob = 0.8 | step_down = None\n",
    "Mario is little drunk, so it will try to avoid right path, so that chances of falling in pit is less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "\n",
      "values:\n",
      "---------------------------\n",
      " 0.48| 0.63| 0.77| 0.00|\n",
      "---------------------------\n",
      " 0.39| 0.00| 0.44| 0.00|\n",
      "---------------------------\n",
      " 0.30| 0.24| 0.30| 0.20|\n",
      "\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  L  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  U  |  D  |\n"
     ]
    }
   ],
   "source": [
    "grid = standard_grid(obey_prob=0.5, step_cost=None)\n",
    "print_game(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### obey_prob = 0.5 | step_down = None\n",
    "Mario is lot drunk, it will try to follow left path as it is safe. and on (1,2) it will start bumping into wall just to avoid pit. We can try to predict best move there\n",
    "* up = (0.5x0 + 0.5x0.9x0.77) + (0.25x0+ 0.25x0.9x.44) + (0.25x-1 + 0.25x0.9x0 ) = 0.3465+0.099-1 = -0.5545\n",
    "* left = (0.5x0 + 0.5x0.9x0.44 )+ (0.25x0+ 0.25x0.9x0.77) + (0.25x0+ 0.25x0.9x0.30) = 0.198+0.17325+0.0675 = 0.43875\n",
    "* So, Left seem to be more rewarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "\n",
      "values:\n",
      "---------------------------\n",
      "-0.03| 0.27| 0.55| 0.00|\n",
      "---------------------------\n",
      "-0.21| 0.00|-0.10| 0.00|\n",
      "---------------------------\n",
      "-0.39|-0.50|-0.38|-0.57|\n",
      "\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  D  |\n"
     ]
    }
   ],
   "source": [
    "grid = standard_grid(obey_prob=0.5, step_cost=-0.1)\n",
    "print_game(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### obey_prob = 0.5 | step_down = -0.1\n",
    "Mario is still lot drunk but it will have to reach goal as soon as possible, it will try to follow left path as it is safe. This time mario will take a risk on (1,2). We can try to predict best move there\n",
    "* up = (0.5x-0.1 + 0.5x0.9x0.55) + (0.25x-.10+ 0.25x0.9x-.10) + (0.25x-1 + 0.25x0.9x0 ) = 0.2965-0.0475-0.25 = -0.001\n",
    "* left = (0.5x-0.1 + 0.5x0.9x-0.1 )+ (0.25x-0.1+ 0.25x0.9x0.55) + (0.25x-0.1+ 0.25x0.9x-0.38) = -0.095+0.09875-0.1105 = -0.10675\n",
    "* So, here up seems to be more rewarding\n",
    "* weird thing happened at (2,3), mario seem to prefer in stay in same state rather than moving, as staying is more rewarding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.50|-0.50|-0.50| 1.00|\n",
      "---------------------------\n",
      "-0.50| 0.00|-0.50|-1.00|\n",
      "---------------------------\n",
      "-0.50|-0.50|-0.50|-0.50|\n",
      "\n",
      "values:\n",
      "---------------------------\n",
      "-1.82|-0.82| 0.11| 0.00|\n",
      "---------------------------\n",
      "-2.40| 0.00|-0.74| 0.00|\n",
      "---------------------------\n",
      "-2.66|-2.28|-1.68|-1.45|\n",
      "\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  U  |\n"
     ]
    }
   ],
   "source": [
    "grid = standard_grid(obey_prob=0.5, step_cost=-0.5)\n",
    "print_game(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### obey_prob = 0.5 | step_down = -0.5\n",
    "Mario is still lot drunk but it will have to reach goal as soon as possible such that again a weired thing happened at (2,3), mario decided to commit sucide. Lets see why\n",
    "* up = ( 0.5x-1 + 0.5x0.9x0 )+( 0.25x-0.5 + 0.25x0.9x-1.68 )+(0.25x-0.5+0.25x0.9x-1.45) = -0.5-0.503-0.45125 = -1.45425\n",
    "* down = (0.5x-0.5+0.5x0.9x-1.45)+(0.25x-0.5+0.25x0.9x-1.45)+(0.25x-0.5+0.25x0.9x-1.68) = -0.9025-0.45125-0.503 =-1.85675\n",
    "* As it is clear, sucide is best option for mario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-2.00|-2.00|-2.00| 1.00|\n",
      "---------------------------\n",
      "-2.00| 0.00|-2.00|-1.00|\n",
      "---------------------------\n",
      "-2.00|-2.00|-2.00|-2.00|\n",
      "\n",
      "values:\n",
      "---------------------------\n",
      "-2.99|-1.10| 1.00| 0.00|\n",
      "---------------------------\n",
      "-4.69| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      "-6.15|-4.61|-2.90|-1.00|\n",
      "\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  U  |\n"
     ]
    }
   ],
   "source": [
    "grid = standard_grid(obey_prob=1, step_cost=-2)\n",
    "print_game(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### obey_prob = 0.5 | step_down = -2\n",
    "Mario is still lot drunk but it will have to reach goal as soon as possible and such that moving one extra tile is having more penalty which cant be compensated by any future reward. So here also mario prefers to die in some of the situations. Poor Mario!!!...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
